ğŸ§¬ COVID-19 Data Pipeline
ğŸ“Œ VisÃ£o Geral

Este projeto implementa um pipeline de dados completo para coletar, processar e analisar informaÃ§Ãµes sobre casos de COVID-19.
O objetivo Ã© demonstrar habilidades prÃ¡ticas de engenharia de dados, incluindo:

    Coleta automatizada de dados de fontes pÃºblicas (APIs e CSVs)

    Limpeza, transformaÃ§Ã£o e enriquecimento dos dados

    Armazenamento otimizado em banco de dados relacional e/ou data lake

    CriaÃ§Ã£o de relatÃ³rios e dashboards para anÃ¡lise exploratÃ³ria

ğŸ§  Objetivo do Projeto

O projeto foi desenvolvido para:

    Praticar conceitos fundamentais de ETL/ELT

    Aplicar boas prÃ¡ticas de modelagem e versionamento de dados

    Aprender a orquestrar tarefas de dados de forma automatizada

    Servir como base para futuros pipelines em escala

âš™ï¸ Arquitetura do Pipeline
    A[ğŸ“¥ Coleta de Dados API / CSV] --> B[ğŸ§¹ Limpeza e TransformaÃ§Ã£o (Pandas / PySpark)]
    B --> C[(ğŸ’¾ Armazenamento PostgreSQL / Data Lake)]
    C --> D[ğŸ“Š VisualizaÃ§Ã£o Power BI / Metabase / Tableau]


Etapas:
    IngestÃ£o: dados coletados diariamente da API COVID-19 Dataset (https://www.kaggle.com/datasets/imdevskp/corona-virus-report)

    TransformaÃ§Ã£o: padronizaÃ§Ã£o de nomes, datas, paÃ­ses e cÃ¡lculo de mÃ©tricas derivadas (mÃ©dias mÃ³veis, taxa de mortalidade, etc).

    Carga: dados salvos em banco relacional (ex: PostgreSQL) e exportados para formato parquet no Data Lake.

    VisualizaÃ§Ã£o: criaÃ§Ã£o de painÃ©is interativos para acompanhamento da evoluÃ§Ã£o global da pandemia.

ğŸ§° Tecnologias Utilizadas
Linguagem:	Python
Coleta:	requests, pandas
TransformaÃ§Ã£o:	pandas, numpy
<!-- OrquestraÃ§Ã£o:	Apache Airflow  - futuramente -->
Armazenamento:	PostgreSQL / SQLite 
VisualizaÃ§Ã£o:	Power BI / Tableau
Controle de versÃ£o:	Git + GitHub
<!-- Ambiente	Docker - futuramente -->
ğŸ§© Estrutura do Projeto
covid19-pipeline/
â”‚

â”œâ”€â”€ data/                  # Dados brutos e transformados

â”œâ”€â”€ dags/                  # (futuramente) DAGs do Airflow

â”œâ”€â”€ notebooks/             # Jupyter notebooks de exploraÃ§Ã£o

â”œâ”€â”€ src/

â”‚   â”œâ”€â”€ extract.py         # Coleta e ingestÃ£o dos dados

â”‚   â”œâ”€â”€ transform.py       # Limpeza e transformaÃ§Ã£o

â”‚   â”œâ”€â”€ load.py            # Armazenamento e carga final

â”‚   â””â”€â”€ utils.py           # FunÃ§Ãµes auxiliares

â”œâ”€â”€ venv/                  # ConfiguraÃ§Ã£o do ambiente virtual

â”œâ”€â”€ requirements.txt       # DependÃªncias do projeto

â”œâ”€â”€ Dockerfile             # (futuramente) ConfiguraÃ§Ã£o do container

â”œâ”€â”€ README.md              # DocumentaÃ§Ã£o

â””â”€â”€ dashboard.pbix         # Painel (Power BI ou Metabase export)

ğŸš€ Como Executar Localmente
# 1. Clone o repositÃ³rio
git clone https://github.com/mlugard/covid19-pipeline.git
cd covid19-pipeline

# 2. Crie o ambiente virtual
python -m venv venv
source venv/bin/activate  # ou venv\Scripts\activate no Windows

# 3. Instale as dependÃªncias
pip install -r requirements.txt

# 4. Execute o pipeline
python src/extract.py
python src/transform.py
python src/load.py


ğŸ’¡ Caso utilize o Airflow, configure as DAGs no diretÃ³rio dags/ e execute airflow webserver para orquestrar o pipeline.

ğŸ“ˆ Exemplo de Resultados

MÃ©dia mÃ³vel de novos casos por paÃ­s

Taxa de mortalidade ao longo do tempo

Ranking de paÃ­ses por vacinaÃ§Ã£o

Dashboard interativo (Power BI ou Metabase)

ğŸ§¾ PrÃ³ximos Passos

 Adicionar integraÃ§Ã£o com API de vacinaÃ§Ã£o

 Automatizar execuÃ§Ã£o diÃ¡ria via Airflow

 Migrar armazenamento para AWS S3 + Redshift

 Disponibilizar API de consulta de dados tratados

ğŸ‘¨â€ğŸ’» Autor

Marcelo Cabral
ğŸ“ Engenharia de Dados | Foco em Projetos Reais e Cloud

# ğŸ”— LinkedIn
